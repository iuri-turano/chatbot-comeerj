# ğŸ¤– CLAUDE.md - Assistente EspÃ­rita

## ğŸ“‹ VisÃ£o Geral do Projeto

**Chatbot EspÃ­rita com RAG (Retrieval-Augmented Generation)** - Um assistente conversacional inteligente especializado em obras espÃ­ritas que utiliza busca semÃ¢ntica e geraÃ§Ã£o de respostas baseadas em fontes autÃªnticas.

### ğŸ¯ Objetivo Principal

Criar uma experiÃªncia similar ao **Perplexity Chat** onde:
- O usuÃ¡rio vÃª **em tempo real** o que o sistema estÃ¡ fazendo
- Mostra quais **livros estÃ£o sendo consultados**
- Exibe as **fontes** das informaÃ§Ãµes com priorizaÃ§Ã£o inteligente
- Oferece **respostas contextualizadas** baseadas em obras espÃ­ritas autÃªnticas

### ğŸŒ Idioma e LocalizaÃ§Ã£o

- **Interface do UsuÃ¡rio**: PORTUGUÃŠS BRASILEIRO (pt-BR)
- **Respostas da IA**: SEMPRE em PORTUGUÃŠS BRASILEIRO
- **Livros**: Obras espÃ­ritas em portuguÃªs (CodificaÃ§Ã£o de Allan Kardec)

### âœ¨ CaracterÃ­sticas Principais

#### 1. InteligÃªncia Contextual
- âœ… **Identifica e recusa perguntas FORA DE CONTEXTO** (nÃ£o relacionadas ao Espiritismo) - **IMPLEMENTADO 2025-02-01**
- âœ… **Correlaciona contexto** atravÃ©s do histÃ³rico de conversa
- ğŸ”´ **MÃºltiplas buscas automÃ¡ticas** quando necessÃ¡rio para respostas mais completas - **PENDENTE** (ver [proposta 002](docs/proposed/002-multiple-search-capability.md))
- âœ… **PriorizaÃ§Ã£o inteligente de fontes** (O Livro dos EspÃ­ritos tem peso mÃ¡ximo)

#### 2. Interface Estilo Perplexity
- ğŸŸ¡ **Indicadores de processo em tempo real** - **50% IMPLEMENTADO** (backend pronto, frontend pendente):
  - "Criando modelo LLM..." (10% concluÃ­do) - Backend âœ… | Frontend ğŸ”´
  - "Buscando nos livros espÃ­ritas..." (30% concluÃ­do) - Backend âœ… | Frontend ğŸ”´
  - "Construindo contexto..." (50% concluÃ­do) - Backend âœ… | Frontend ğŸ”´
  - "Gerando resposta..." (70% concluÃ­do) - Backend âœ… | Frontend ğŸ”´
  - "Formatando resposta..." (90% concluÃ­do) - Backend âœ… | Frontend ğŸ”´
  - Ver [proposta 003](docs/proposed/003-real-time-progress-indicators.md)
- âœ… **Exibe fontes consultadas** com badges de prioridade
- âœ… **Streaming de respostas** (texto aparece progressivamente)
- âœ… **Status do backend** visÃ­vel para o usuÃ¡rio

#### 3. Sistema de PriorizaÃ§Ã£o de Fontes

**Hierarquia de Prioridades:**

| Prioridade | Peso | Obras | Badge |
|-----------|------|-------|-------|
| ğŸ¥‡ **MÃXIMA** | 100 | O Livro dos EspÃ­ritos | `PRIORIDADE MÃXIMA` |
| ğŸ¥ˆ **FUNDAMENTAL** | 70 | Evangelho, MÃ©diuns, GÃªnese, CÃ©u e Inferno, O que Ã© o Espiritismo | `OBRA FUNDAMENTAL` |
| ğŸ¥‰ **COMPLEMENTAR** | 40 | Revista EspÃ­rita (1858-1869) | `COMPLEMENTAR` |
| ğŸ“„ **OUTRAS** | 10 | Demais obras | `OUTRAS OBRAS` |

## ğŸ—ï¸ Arquitetura do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (Streamlit)                      â”‚
â”‚  - Interface em PortuguÃªs BR                                 â”‚
â”‚  - Chat com histÃ³rico contextual                             â”‚
â”‚  - Indicadores de processo em tempo real                     â”‚
â”‚  - ExibiÃ§Ã£o de fontes com badges de prioridade              â”‚
â”‚  - Sistema de feedback                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ HTTP/API REST
                  â”‚ /query (normal) ou /query_stream (streaming)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BACKEND (FastAPI)                           â”‚
â”‚  - API REST com endpoints de status                          â”‚
â”‚  - Sistema de rastreamento de tarefas                        â”‚
â”‚  - ValidaÃ§Ã£o de contexto (rejeita perguntas off-topic)       â”‚
â”‚  - MÃºltiplas buscas automÃ¡ticas se necessÃ¡rio                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChromaDB     â”‚  â”‚   Ollama    â”‚  â”‚  PyTorch     â”‚
â”‚  (Vector DB)   â”‚  â”‚  (LLM Local)â”‚  â”‚  (Embeddings)â”‚
â”‚  - Embeddings  â”‚  â”‚  - Qwen2.5  â”‚  â”‚  - GPU/CPU   â”‚
â”‚  - Busca       â”‚  â”‚  - 7B paramsâ”‚  â”‚  - CUDA/MPS  â”‚
â”‚    SemÃ¢ntica   â”‚  â”‚             â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Processamento

1. **UsuÃ¡rio faz pergunta** â†’ Frontend envia para `/query_stream`
2. **Backend analisa contexto** â†’ Verifica se pergunta Ã© relevante ao Espiritismo
3. **Busca semÃ¢ntica** â†’ ChromaDB retorna top-K trechos mais relevantes
4. **PriorizaÃ§Ã£o** â†’ Sistema reordena resultados por prioridade de fonte
5. **GeraÃ§Ã£o com contexto** â†’ LLM gera resposta baseada nos trechos + histÃ³rico
6. **Streaming** â†’ Resposta enviada token por token para o frontend
7. **ExibiÃ§Ã£o de fontes** â†’ UsuÃ¡rio vÃª de quais livros vieram as informaÃ§Ãµes

## ğŸ› ï¸ Stack TecnolÃ³gico

### Backend
- **FastAPI** - Framework web assÃ­ncrono
- **Ollama** - Servidor LLM local (roda Qwen2.5:7b)
- **ChromaDB** - Banco de dados vetorial
- **LangChain** - Framework para RAG
- **Sentence Transformers** - Embeddings multilÃ­ngues (paraphrase-multilingual-mpnet-base-v2)
- **PyTorch** - AceleraÃ§Ã£o GPU (CUDA para NVIDIA, MPS para Apple Silicon)

### Frontend
- **Streamlit** - Interface web interativa
- **Requests** - Cliente HTTP para API

### Modelo LLM
- **Qwen2.5:7b** - Modelo otimizado para portuguÃªs (padrÃ£o)
- Suporta outros: llama3.2:3b, llama3.2:1b

## ğŸ“¦ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

#### Para Windows (NVIDIA GPU)
- Python 3.11+
- NVIDIA GPU (RTX 3070 ou superior)
- NVIDIA CUDA Toolkit 11.8+
- Ollama for Windows

#### Para Mac (Apple Silicon)
- Python 3.11+
- Mac com chip M1/M2/M3/M4
- Ollama for macOS

### 1ï¸âƒ£ InstalaÃ§Ã£o do Ollama

#### Windows:
```bash
# Baixar de: https://ollama.com/download/windows
# Instalar e verificar
ollama --version

# Baixar modelo
ollama pull qwen2.5:7b
```

#### Mac:
```bash
# Baixar de: https://ollama.com/download/mac
# Ou via Homebrew:
brew install ollama

# Baixar modelo
ollama pull qwen2.5:7b
```

### 2ï¸âƒ£ Setup do Backend

#### Windows (NVIDIA 3070):

```bash
cd backend

# Criar ambiente virtual
python -m venv venv
venv\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar PyTorch com CUDA (para GPU NVIDIA)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Verificar CUDA
python -c "import torch; print(f'CUDA disponÃ­vel: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

#### Mac (M4 Pro):

```bash
cd backend

# Criar ambiente virtual
python3 -m venv venv
source venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt

# PyTorch jÃ¡ vem com suporte MPS (Metal Performance Shaders)
# Verificar MPS
python -c "import torch; print(f'MPS disponÃ­vel: {torch.backends.mps.is_available()}')"
```

### 3ï¸âƒ£ Adicionar Livros

```bash
# Copiar PDFs dos livros espÃ­ritas para a pasta books/
# Estrutura esperada:
backend/books/
  â”œâ”€â”€ Livro-dos-Espiritos.pdf
  â”œâ”€â”€ O-evangelho-segundo-o-espiritismo.pdf
  â”œâ”€â”€ Livro-dos-Mediuns_Guillon.pdf
  â”œâ”€â”€ A-genese_Guillon.pdf
  â”œâ”€â”€ ceu-e-inferno-Manuel-Quintao.pdf
  â””â”€â”€ ... (outros livros)
```

### 4ï¸âƒ£ Processar Livros

```bash
# Windows
python process_books.py

# Mac
python3 process_books.py

# Este processo:
# - Carrega todos os PDFs da pasta books/
# - Divide em chunks de 1000 caracteres
# - Cria embeddings usando GPU/MPS
# - Salva no ChromaDB (pasta database/)
# Tempo estimado: 10-20 minutos
```

### 5ï¸âƒ£ Setup do Frontend

#### Windows:
```bash
cd frontend

# Criar ambiente virtual
python -m venv venv
venv\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar API URL (para uso local)
mkdir .streamlit
echo API_URL = "http://localhost:8000" > .streamlit\secrets.toml
```

#### Mac:
```bash
cd frontend

# Criar ambiente virtual
python3 -m venv venv
source venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar API URL (para uso local)
mkdir -p .streamlit
echo 'API_URL = "http://localhost:8000"' > .streamlit/secrets.toml
```

## ğŸš€ Scripts de ExecuÃ§Ã£o

### Windows (NVIDIA 3070)

#### Backend - `start_backend.bat`
```batch
@echo off
echo ========================================
echo   BACKEND - ASSISTENTE ESPIRITA
echo ========================================
echo.

cd /d %~dp0

echo [1/3] Ativando ambiente virtual...
call venv\Scripts\activate

echo [2/3] Verificando Ollama...
ollama list

echo [3/3] Iniciando API...
python api_server.py

pause
```

#### Frontend - `start_frontend.bat`
```batch
@echo off
echo ========================================
echo   FRONTEND - ASSISTENTE ESPIRITA
echo ========================================
echo.

cd /d %~dp0

echo [1/2] Ativando ambiente virtual...
call venv\Scripts\activate

echo [2/2] Iniciando Streamlit...
streamlit run app.py

pause
```

#### Completo - `start_all.bat`
```batch
@echo off
echo ========================================
echo   ASSISTENTE ESPIRITA - STARTUP COMPLETO
echo ========================================
echo.

echo Iniciando Backend...
start cmd /k "cd backend && call start_backend.bat"

timeout /t 5

echo Iniciando Frontend...
start cmd /k "cd frontend && call start_frontend.bat"

echo.
echo ========================================
echo   Sistema inicializado!
echo   Backend: http://localhost:8000
echo   Frontend: http://localhost:8501
echo ========================================
```

### Mac (M4 Pro)

#### Backend - `start_backend.sh`
```bash
#!/bin/bash

echo "========================================"
echo "  BACKEND - ASSISTENTE ESPIRITA"
echo "========================================"
echo ""

# Get script directory
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$DIR"

echo "[1/3] Ativando ambiente virtual..."
source venv/bin/activate

echo "[2/3] Verificando Ollama..."
ollama list

echo "[3/3] Iniciando API..."
python api_server.py
```

#### Frontend - `start_frontend.sh`
```bash
#!/bin/bash

echo "========================================"
echo "  FRONTEND - ASSISTENTE ESPIRITA"
echo "========================================"
echo ""

# Get script directory
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$DIR"

echo "[1/2] Ativando ambiente virtual..."
source venv/bin/activate

echo "[2/2] Iniciando Streamlit..."
streamlit run app.py
```

#### Completo - `start_all.sh`
```bash
#!/bin/bash

echo "========================================"
echo "  ASSISTENTE ESPIRITA - STARTUP COMPLETO"
echo "========================================"
echo ""

# Start backend in new terminal
osascript -e 'tell app "Terminal" to do script "cd \"'$(pwd)'/backend\" && ./start_backend.sh"'

echo "Aguardando backend iniciar..."
sleep 5

# Start frontend in new terminal
osascript -e 'tell app "Terminal" to do script "cd \"'$(pwd)'/frontend\" && ./start_frontend.sh"'

echo ""
echo "========================================"
echo "  Sistema inicializado!"
echo "  Backend: http://localhost:8000"
echo "  Frontend: http://localhost:8501"
echo "========================================"
```

## ğŸ® Como Usar

### 1. Iniciar o Sistema

**Windows:**
```bash
# OpÃ§Ã£o 1: Tudo de uma vez
start_all.bat

# OpÃ§Ã£o 2: Separado
cd backend
start_backend.bat
# Em outro terminal:
cd frontend
start_frontend.bat
```

**Mac:**
```bash
# Tornar scripts executÃ¡veis (primeira vez)
chmod +x start_all.sh backend/start_backend.sh frontend/start_frontend.sh

# OpÃ§Ã£o 1: Tudo de uma vez
./start_all.sh

# OpÃ§Ã£o 2: Separado
cd backend
./start_backend.sh
# Em outro terminal:
cd frontend
./start_frontend.sh
```

### 2. Acessar a Interface

Abrir navegador em: **http://localhost:8501**

### 3. Fazer Perguntas

**Exemplos de perguntas vÃ¡lidas (IN CONTEXT):**
- "O que Ã© o perispÃ­rito?"
- "Explique sobre a reencarnaÃ§Ã£o segundo o Espiritismo"
- "O que Allan Kardec diz sobre a mediunidade?"
- "Qual a diferenÃ§a entre mÃ©dium e sensitivo?"

**Exemplos de perguntas FORA DE CONTEXTO (serÃ£o recusadas):**
- "Qual Ã© a previsÃ£o do tempo?"
- "Como fazer um bolo de chocolate?"
- "Quem ganhou a Copa do Mundo?"

### 4. Ver Progresso em Tempo Real

Durante o processamento, vocÃª verÃ¡:
```
ğŸ” Consultando os livros...
â”œâ”€ [10%] Criando modelo LLM
â”œâ”€ [30%] Buscando nos livros espÃ­ritas
â”œâ”€ [50%] Construindo contexto
â”œâ”€ [70%] Gerando resposta
â””â”€ [90%] Formatando resposta
```

### 5. Analisar Fontes

ApÃ³s cada resposta, expanda "ğŸ“– Fontes Consultadas" para ver:
- ğŸ¥‡ **PRIORIDADE MÃXIMA** - O Livro dos EspÃ­ritos
- ğŸ¥ˆ **OBRA FUNDAMENTAL** - Evangelho, MÃ©diuns, etc.
- ğŸ¥‰ **COMPLEMENTAR** - Revista EspÃ­rita
- ğŸ“„ **OUTRAS OBRAS** - Demais livros

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Ajustar Prioridades

Editar `backend/config.py`:

```python
BOOK_PRIORITIES = {
    "livro-dos-espiritos.pdf": 100,  # Peso mÃ¡ximo
    "evangelho": 70,                  # Fundamental
    "mediuns": 70,
    # Adicionar novas prioridades...
}
```

### Alterar Modelo LLM

```bash
# Baixar novo modelo
ollama pull llama3.2:3b

# Na interface Streamlit (sidebar):
# Selecionar modelo desejado no dropdown
```

### Ajustar ParÃ¢metros de Busca

Na interface Streamlit:
- **Temperatura** (0.0 - 1.0): Controla criatividade
  - 0.1-0.3: Mais fiel aos textos (recomendado)
  - 0.7-1.0: Mais criativo
- **NÂº de trechos** (1-10): Quantos trechos usar
  - 3-5: PadrÃ£o (recomendado)
  - 8-10: Mais contexto (mais lento)
- **Busca inicial** (fetch_k): Quantos trechos buscar antes de priorizar
  - 15: PadrÃ£o
  - 20-30: Busca mais ampla

### Modificar Chunk Size

Editar `backend/config.py`:

```python
CHUNK_SIZE = 1000      # Tamanho em caracteres
CHUNK_OVERLAP = 200    # SobreposiÃ§Ã£o
```

**Depois reprocessar:**
```bash
cd backend
python process_books.py
```

## ğŸ§ª Testes

### Testar Backend

```bash
# Health check
curl http://localhost:8000/health

# Status detalhado
curl http://localhost:8000/status/detailed

# Fazer pergunta via API
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "O que Ã© o perispÃ­rito?",
    "model_name": "qwen2.5:7b",
    "temperature": 0.3,
    "top_k": 3,
    "fetch_k": 15
  }'
```

### Testar Frontend

1. Acessar http://localhost:8501
2. Verificar status do backend (sidebar)
3. Fazer pergunta de teste
4. Verificar fontes retornadas
5. Testar feedback (ğŸ‘ğŸ˜ğŸ‘)

## ğŸ“Š Endpoints da API

### Status Endpoints

| Endpoint | MÃ©todo | DescriÃ§Ã£o |
|----------|--------|-----------|
| `/` | GET | Health check rÃ¡pido |
| `/health` | GET | Ultra-lightweight health check |
| `/status` | GET | Status quick (idÃªntico a `/`) |
| `/status/detailed` | GET | Status detalhado com tasks ativas |
| `/status/task/{task_id}` | GET | Status de uma task especÃ­fica |
| `/status/history` | GET | HistÃ³rico de requests |

### Query Endpoints

| Endpoint | MÃ©todo | DescriÃ§Ã£o |
|----------|--------|-----------|
| `/query` | POST | Processa pergunta (resposta completa) |
| `/query_stream` | POST | Processa com streaming (estilo Perplexity) |

## ğŸ” DetecÃ§Ã£o de Contexto (Out-of-Context Detection)

O sistema identifica perguntas fora de contexto atravÃ©s de:

1. **AnÃ¡lise semÃ¢ntica**: Embeddings da pergunta comparados com corpus espÃ­rita
2. **Threshold de relevÃ¢ncia**: Score mÃ­nimo de similaridade
3. **ValidaÃ§Ã£o de tÃ³picos**: Keywords relacionadas ao Espiritismo
4. **HistÃ³rico de conversa**: Contexto acumulado das mensagens

**Comportamento:**
```python
# Pergunta IN CONTEXT
"O que Ã© reencarnaÃ§Ã£o?" â†’ Processa normalmente

# Pergunta OUT OF CONTEXT
"Qual a receita de bolo?" â†’ Responde:
"Desculpe, sÃ³ posso responder perguntas sobre Espiritismo
e Doutrina EspÃ­rita. Por favor, faÃ§a uma pergunta relacionada
Ã s obras de Allan Kardec."
```

## ğŸ¤ Sistema de Feedback

Cada resposta pode ser avaliada:
- ğŸ‘ **Boa**: Resposta Ãºtil e precisa
- ğŸ˜ **Regular**: Resposta ok mas pode melhorar
- ğŸ‘ **Ruim**: Resposta inadequada

Feedback Ã© salvo em `frontend/feedback.jsonl` para anÃ¡lise.

## ğŸ“ Estrutura de Arquivos

```
chatbot-comeerj/
â”œâ”€â”€ CLAUDE.md                    # Este arquivo
â”œâ”€â”€ README.md                    # DocumentaÃ§Ã£o geral
â”œâ”€â”€ FEEDBACK_GUIDE.md           # Guia de feedback
â”œâ”€â”€ LICENSE
â”œâ”€â”€ start_all.bat               # Windows: Inicia tudo
â”œâ”€â”€ start_all.sh                # Mac: Inicia tudo
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api_server.py           # API FastAPI
â”‚   â”œâ”€â”€ config.py               # ConfiguraÃ§Ãµes e prioridades
â”‚   â”œâ”€â”€ priority_retriever.py  # Sistema de priorizaÃ§Ã£o
â”‚   â”œâ”€â”€ process_books.py        # Processa PDFs
â”‚   â”œâ”€â”€ feedback_system.py     # Sistema de feedback
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ start_backend.bat       # Windows
â”‚   â”œâ”€â”€ start_backend.sh        # Mac
â”‚   â”œâ”€â”€ books/                  # PDFs (nÃ£o versionado)
â”‚   â”œâ”€â”€ database/               # ChromaDB (nÃ£o versionado)
â”‚   â””â”€â”€ venv/                   # Python venv
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ app.py                  # Interface Streamlit
    â”œâ”€â”€ chat_history.py         # Gerenciamento de conversas
    â”œâ”€â”€ feedback_system.py     # Sistema de feedback
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ start_frontend.bat      # Windows
    â”œâ”€â”€ start_frontend.sh       # Mac
    â”œâ”€â”€ .streamlit/
    â”‚   â””â”€â”€ secrets.toml        # Config API URL (nÃ£o versionado)
    â””â”€â”€ venv/                   # Python venv
```

## ğŸ› Troubleshooting

### Backend nÃ£o inicia

**Windows:**
```bash
# Verificar Ollama
ollama list

# Se nÃ£o funcionar, iniciar Ollama
# Procurar "Ollama" no menu iniciar e executar

# Verificar CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Se False, reinstalar PyTorch com CUDA
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**Mac:**
```bash
# Verificar Ollama
ollama list

# Se nÃ£o funcionar, iniciar Ollama
ollama serve &

# Verificar MPS
python -c "import torch; print(torch.backends.mps.is_available())"
```

### Frontend nÃ£o conecta

```bash
# Verificar se backend estÃ¡ rodando
curl http://localhost:8000/health

# Verificar secrets.toml
cat frontend/.streamlit/secrets.toml

# Deve conter:
# API_URL = "http://localhost:8000"
```

### Respostas ruins

1. **Reduzir temperatura** para 0.1-0.2
2. **Aumentar top_k** para 5-8 trechos
3. **Aumentar fetch_k** para 20-30
4. **Testar outro modelo**: llama3.2:3b

### Banco corrompido

```bash
cd backend

# Windows
rmdir /s database
python process_books.py

# Mac
rm -rf database
python process_books.py
```

## âš¡ Performance

### Benchmarks

**Windows (NVIDIA RTX 3070):**
- Processamento inicial: ~15min para 23k chunks
- Busca vetorial: ~200ms
- GeraÃ§Ã£o de resposta: 2-5s
- Throughput: 10-15 perguntas/min

**Mac (M4 Pro):**
- Processamento inicial: ~20min para 23k chunks
- Busca vetorial: ~300ms
- GeraÃ§Ã£o de resposta: 3-6s
- Throughput: 8-12 perguntas/min

## ğŸ“ Desenvolvimento

### Adicionar Nova Funcionalidade

1. **Backend**: Editar `backend/api_server.py`
2. **Frontend**: Editar `frontend/app.py`
3. **ConfiguraÃ§Ã£o**: Editar `backend/config.py`
4. **Testes**: Testar via curl e interface

### Debugging

**Backend:**
```python
# Adicionar logs em api_server.py
print(f"DEBUG: {variavel}")

# Acessar logs
# Windows: Ver console do backend
# Mac: Ver terminal do backend
```

**Frontend:**
```python
# Adicionar em app.py
st.write(f"DEBUG: {variavel}")

# Aparece na interface
```

### Adicionar Novo Modelo

```bash
# Baixar modelo
ollama pull nome-do-modelo

# Editar frontend/app.py
model_name = st.selectbox(
    "Modelo:",
    ["qwen2.5:7b", "llama3.2:3b", "nome-do-modelo"]
)
```

## ğŸ” SeguranÃ§a e Privacidade

- âœ… **100% Local**: Nenhum dado enviado para servidores externos
- âœ… **GPU/CPU Local**: Processamento totalmente on-premise
- âœ… **Sem internet necessÃ¡ria** (apÃ³s setup inicial)
- âœ… **Dados privados**: Conversas salvas localmente

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [FastAPI](https://fastapi.tiangolo.com/)
- [Ollama](https://ollama.com/)
- [ChromaDB](https://docs.trychroma.com/)
- [LangChain](https://python.langchain.com/)
- [Streamlit](https://docs.streamlit.io/)
- [PyTorch](https://pytorch.org/)

## ğŸ¤ Contribuindo

Para contribuir:
1. Use o sistema normalmente
2. Avalie respostas (ğŸ‘ğŸ˜ğŸ‘)
3. Deixe comentÃ¡rios detalhados
4. Reporte bugs via issues

## ğŸ“„ LicenÃ§a

MIT License - Uso educacional e religioso.

Obras espÃ­ritas utilizadas sÃ£o de domÃ­nio pÃºblico (Allan Kardec).

## âœ¨ CrÃ©ditos

- Allan Kardec - CodificaÃ§Ã£o EspÃ­rita
- FEB - FederaÃ§Ã£o EspÃ­rita Brasileira
- Comunidade Open Source

---

## ğŸ“Š Status de ImplementaÃ§Ã£o das Funcionalidades

| Funcionalidade | Status | Detalhes |
|---------------|--------|----------|
| Out-of-Context Detection | âœ… **IMPLEMENTADO** | Sistema de 3 camadas validando perguntas (2025-02-01) |
| Context Correlation | âœ… **IMPLEMENTADO** | HistÃ³rico de conversa funcional |
| Multiple Search | ğŸ”´ **PENDENTE** | Ver [proposta 002](docs/proposed/002-multiple-search-capability.md) |
| Real-Time Progress | ğŸŸ¡ **50% PRONTO** | Backend completo, frontend pendente ([proposta 003](docs/proposed/003-real-time-progress-indicators.md)) |
| Source Prioritization | âœ… **IMPLEMENTADO** | Sistema de prioridades funcionando |
| Streaming Responses | âœ… **IMPLEMENTADO** | Streaming via SSE funcionando |
| Portuguese UI/AI | âœ… **IMPLEMENTADO** | 100% em portuguÃªs brasileiro |
| Feedback System | âœ… **IMPLEMENTADO** | Sistema de feedback funcionando |

### Arquivos de ImplementaÃ§Ã£o - Out-of-Context Detection

- `backend/context_validator.py` - Validador de contexto
- `backend/config.py` - ConfiguraÃ§Ãµes (threshold: 0.35)
- `backend/api_server.py` - IntegraÃ§Ã£o nos endpoints
- `backend/test_context_validation.py` - Testes
- `backend/README_TESTING.md` - DocumentaÃ§Ã£o de testes
- `docs/completed/001-out-of-context-detection.md` - Proposta completa

### Como Testar Out-of-Context Detection

```bash
cd backend
source venv/bin/activate
python test_context_validation.py
```

Ou via API:
```bash
# Pergunta vÃ¡lida
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "O que Ã© reencarnaÃ§Ã£o?", "model_name": "qwen2.5:7b"}'

# Pergunta invÃ¡lida (serÃ¡ rejeitada)
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Qual a receita de bolo?", "model_name": "qwen2.5:7b"}'
```

---

**VersÃ£o**: 1.2.2
**Ãšltima atualizaÃ§Ã£o**: Fevereiro 2025
**Desenvolvido com**: Claude Sonnet 4.5
