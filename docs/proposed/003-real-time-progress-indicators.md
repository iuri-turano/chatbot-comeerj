# Proposta 003: Indicadores de Progresso em Tempo Real (Estilo Perplexity)

**Status**: üü° PARCIALMENTE IMPLEMENTADO
**Prioridade**: üî∂ ALTA
**Esfor√ßo Estimado**: M√©dio (4-5 horas)
**Impacto**: Alto - UX/UI significativamente melhorada

---

## üìã Resumo

Completar a implementa√ß√£o dos indicadores de progresso em tempo real que mostram ao usu√°rio exatamente o que o sistema est√° fazendo durante o processamento, similar √† experi√™ncia do Perplexity AI.

## üéØ Objetivo Declarado (CLAUDE.md)

> "Interface Estilo Perplexity com indicadores de processo em tempo real"

**Comportamento Esperado (CLAUDE.md linhas 30-35):**
```
üîç Consultando os livros...
‚îú‚îÄ [10%] Criando modelo LLM
‚îú‚îÄ [30%] Buscando nos livros esp√≠ritas
‚îú‚îÄ [50%] Construindo contexto
‚îú‚îÄ [70%] Gerando resposta
‚îî‚îÄ [90%] Formatando resposta
```

**Experi√™ncia Desejada:**
- Usu√°rio v√™ **5 est√°gios distintos** de processamento
- Cada est√°gio mostra **texto descritivo** + **porcentagem**
- **Anima√ß√£o visual** (barra de progresso ou spinner)
- **Transpar√™ncia total** sobre o que est√° acontecendo
- **Feedback cont√≠nuo** durante todo o processamento

## ‚ùå Situa√ß√£o Atual

### ‚úÖ O que est√° IMPLEMENTADO (Backend)

**Backend tem infraestrutura completa:**

1. **Sistema de Status** (`api_server.py` linhas 46-136):
   ```python
   class ServerStatus:
       def update_task(self, task_id: str, stage: str, progress: int)
   ```

2. **Stages Definidos** (`api_server.py`):
   - ‚úÖ "creating_llm" (10%)
   - ‚úÖ "searching_books" (30%)
   - ‚úÖ "building_context" (50%)
   - ‚úÖ "generating_answer" (70%)
   - ‚úÖ "formatting_response" (90%)

3. **Tracking Interno** (linhas 425, 432, 450, 470, 476):
   ```python
   status_tracker.update_task(task_id, "creating_llm", 10)
   status_tracker.update_task(task_id, "searching_books", 30)
   status_tracker.update_task(task_id, "building_context", 50)
   status_tracker.update_task(task_id, "generating_answer", 70)
   status_tracker.update_task(task_id, "formatting_response", 90)
   ```

### ‚ùå O que est√° FALTANDO

**Problema 1: Backend n√£o envia todos os stages para o frontend**

C√≥digo atual (`api_server.py` linhas 549-583):
```python
# Apenas 2 de 5 stages s√£o enviados via streaming:
yield f"data: {json.dumps({'type': 'status', 'stage': 'searching', 'progress': 30})}\n\n"
# ^ Linha 550

yield f"data: {json.dumps({'type': 'status', 'stage': 'generating', 'progress': 70})}\n\n"
# ^ Linha 583

# FALTAM: creating_llm (10%), building_context (50%), formatting_response (90%)
```

**Problema 2: Frontend n√£o exibe os indicadores**

C√≥digo atual (`frontend/app.py` linhas 545, 584):
```python
# Apenas um spinner gen√©rico:
with st.spinner("üîç Consultando os livros..."):
    # Processa resposta mas N√ÉO mostra progresso

# N√£o processa eventos de status:
for chunk, chunk_sources in stream_api_response(...):
    if chunk:
        full_response += chunk
    # N√ÉO verifica se √© evento de status!
```

**Problema 3: stream_api_response n√£o extrai status**

C√≥digo atual (`frontend/app.py` linhas 211-263):
```python
def stream_api_response(...):
    for line in response.iter_lines():
        data = json.loads(line[6:])

        if data['type'] == 'token':
            yield data['content'], None
        elif data['type'] == 'sources':
            sources = data['sources']
        # FALTA: elif data['type'] == 'status'
```

## ‚úÖ Solu√ß√£o Proposta

### Arquitetura de 3 Camadas

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CAMADA 1: BACKEND (api_server.py)                ‚îÇ
‚îÇ  - Yield ALL 5 stages via streaming               ‚îÇ
‚îÇ  - Enviar task_id, stage, progress, description   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ SSE Stream
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CAMADA 2: FRONTEND API (app.py)                  ‚îÇ
‚îÇ  - Parsear eventos de status                      ‚îÇ
‚îÇ  - Separar status, tokens, sources                ‚îÇ
‚îÇ  - Yield tuplas (chunk, sources, status)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ Generator
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CAMADA 3: FRONTEND UI (app.py)                   ‚îÇ
‚îÇ  - Exibir barra de progresso                      ‚îÇ
‚îÇ  - Mostrar texto do stage atual                   ‚îÇ
‚îÇ  - Animar transi√ß√µes entre stages                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementa√ß√£o Detalhada

#### **PARTE 1: Backend - Enviar Todos os Stages**

```python
# backend/api_server.py

# Linha ~538 (modificar fun√ß√£o query_stream)

@app.post("/query_stream")
async def query_stream(request: QueryRequest):
    """Process a question and stream the response with status tracking"""

    if vectorstore is None:
        raise HTTPException(status_code=503, detail="Banco de dados n√£o carregado.")

    task_id = status_tracker.start_request(request.question, mode="streaming")

    async def generate():
        try:
            # Send task_id first
            yield f"data: {json.dumps({'type': 'task_id', 'task_id': task_id})}\n\n"

            # STAGE 1: Creating LLM (10%)
            status_tracker.update_task(task_id, "creating_llm", 10)
            yield f"data: {json.dumps({
                'type': 'status',
                'stage': 'creating_llm',
                'progress': 10,
                'description': 'Criando modelo LLM'
            })}\n\n"

            llm, prompt_template = create_llm_and_prompt(
                request.model_name,
                request.temperature
            )

            # STAGE 2: Searching books (30%)
            status_tracker.update_task(task_id, "searching_books", 30)
            yield f"data: {json.dumps({
                'type': 'status',
                'stage': 'searching_books',
                'progress': 30,
                'description': 'Buscando nos livros esp√≠ritas'
            })}\n\n"

            sources = prioritized_search(
                vectorstore,
                request.question,
                k=request.top_k,
                fetch_k=request.fetch_k
            )

            for source in sources:
                source_path = source.metadata.get('source', '')
                source.metadata['priority'] = get_book_priority(source_path)

            # STAGE 3: Building context (50%)
            status_tracker.update_task(task_id, "building_context", 50)
            yield f"data: {json.dumps({
                'type': 'status',
                'stage': 'building_context',
                'progress': 50,
                'description': 'Construindo contexto'
            })}\n\n"

            context = "\n\n---\n\n".join([
                f"[Trecho {i+1} - {get_book_display_name(doc.metadata.get('source', 'Desconhecido'))}]\n{doc.page_content}"
                for i, doc in enumerate(sources)
            ])

            conversation_context = ""
            if request.conversation_history and len(request.conversation_history) > 0:
                history_text = build_context_with_history(request.conversation_history)
                if history_text:
                    conversation_context = f"\nHIST√ìRICO DA CONVERSA:\n{history_text}\n"

            formatted_prompt = prompt_template.format(
                conversation_context=conversation_context,
                context=context,
                question=request.question
            )

            # STAGE 4: Generating answer (70%)
            status_tracker.update_task(task_id, "generating_answer", 70)
            yield f"data: {json.dumps({
                'type': 'status',
                'stage': 'generating_answer',
                'progress': 70,
                'description': 'Gerando resposta'
            })}\n\n"

            # Stream tokens
            for chunk in llm.stream(formatted_prompt):
                yield f"data: {json.dumps({'type': 'token', 'content': chunk})}\n\n"

            # STAGE 5: Formatting response (90%)
            status_tracker.update_task(task_id, "formatting_response", 90)
            yield f"data: {json.dumps({
                'type': 'status',
                'stage': 'formatting_response',
                'progress': 90,
                'description': 'Formatando resposta'
            })}\n\n"

            # Send sources
            formatted_sources = []
            for source in sources:
                source_path = source.metadata.get('source', 'Desconhecido')
                priority = source.metadata.get('priority', 10)

                if priority >= 100:
                    priority_label = "PRIORIDADE M√ÅXIMA"
                elif priority >= 70:
                    priority_label = "OBRA FUNDAMENTAL"
                elif priority >= 40:
                    priority_label = "COMPLEMENTAR"
                else:
                    priority_label = "OUTRAS OBRAS"

                formatted_sources.append({
                    "content": source.page_content[:500],
                    "source": os.path.basename(source_path),
                    "page": source.metadata.get('page', 0),
                    "priority": priority,
                    "priority_label": priority_label,
                    "display_name": get_book_display_name(source_path)
                })

            yield f"data: {json.dumps({'type': 'sources', 'sources': formatted_sources})}\n\n"

            # COMPLETE (100%)
            yield f"data: {json.dumps({
                'type': 'status',
                'stage': 'complete',
                'progress': 100,
                'description': 'Conclu√≠do'
            })}\n\n"
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

            status_tracker.complete_request(task_id, success=True)

        except Exception as e:
            print(f"‚ùå Erro no streaming: {str(e)}")
            status_tracker.complete_request(task_id, success=False, error=str(e))
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

#### **PARTE 2: Frontend - Parsear Status Events**

```python
# frontend/app.py

# Linha ~211 (modificar stream_api_response)

def stream_api_response(
    question: str,
    model_name: str,
    temperature: float,
    top_k: int,
    fetch_k: int,
    conversation_history: list = None
):
    """Stream response from API with status updates"""
    try:
        # Prepare conversation history
        api_history = []
        if conversation_history:
            for msg in conversation_history:
                api_history.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        response = requests.post(
            f"{API_URL}/query_stream",
            json={
                "question": question,
                "model_name": model_name,
                "temperature": temperature,
                "top_k": top_k,
                "fetch_k": fetch_k,
                "conversation_history": api_history
            },
            stream=True,
            timeout=600
        )
        response.raise_for_status()

        full_text = ""
        sources = None
        current_status = None

        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    import json
                    data = json.loads(line[6:])

                    if data['type'] == 'task_id':
                        # Initial task ID
                        pass

                    elif data['type'] == 'status':
                        # NOVO: Capturar eventos de status
                        current_status = {
                            'stage': data.get('stage'),
                            'progress': data.get('progress'),
                            'description': data.get('description')
                        }
                        yield None, None, current_status  # Yield status

                    elif data['type'] == 'token':
                        full_text += data['content']
                        yield data['content'], None, None  # Yield token

                    elif data['type'] == 'sources':
                        sources = data['sources']
                        # N√£o yield aqui, vai retornar no final

                    elif data['type'] == 'done':
                        # Final: retornar sources
                        yield None, sources, None
                        break

                    elif data['type'] == 'error':
                        raise Exception(data['content'])

    except requests.exceptions.Timeout:
        raise Exception("‚è±Ô∏è Timeout: A resposta demorou muito.")
    except Exception as e:
        raise Exception(f"‚ùå Erro: {str(e)}")
```

#### **PARTE 3: Frontend - Exibir Progress UI**

```python
# frontend/app.py

# Linha ~537 (modificar chat input handling)

# Chat input
if prompt := st.chat_input("Digite sua pergunta sobre Espiritismo..."):
    if not api_status:
        st.error("‚ùå Backend offline. N√£o √© poss√≠vel processar perguntas.")
        return

    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user", avatar="üßë"):
        st.markdown(prompt)

    # Get assistant response
    with st.chat_message("assistant", avatar="ü§ñ"):
        if enable_streaming:
            # NOVO: Progress indicator
            progress_placeholder = st.empty()
            progress_bar = st.progress(0)
            response_placeholder = st.empty()

            full_response = ""
            sources = None
            current_stage = None

            try:
                for chunk, chunk_sources, status_update in stream_api_response(
                    prompt, model_name, temperature, top_k, fetch_k,
                    st.session_state.messages[:-1]
                ):
                    # Handle status updates
                    if status_update:
                        current_stage = status_update
                        progress = status_update['progress']
                        description = status_update['description']

                        # Update progress bar
                        progress_bar.progress(progress / 100)

                        # Update status text
                        progress_placeholder.markdown(
                            f"**üîç {description}** ({progress}%)"
                        )

                    # Handle text chunks
                    elif chunk:
                        # Clear progress display when text starts
                        if current_stage and current_stage['stage'] == 'generating_answer':
                            progress_placeholder.empty()
                            progress_bar.empty()

                        full_response += chunk
                        response_placeholder.markdown(full_response + "‚ñå")

                    # Handle sources
                    elif chunk_sources:
                        sources = chunk_sources

                # Clear progress indicators
                progress_placeholder.empty()
                progress_bar.empty()

                # Final response
                response_placeholder.markdown(full_response)

                # Show sources
                if sources:
                    with st.expander(f"üìñ {len(sources)} Fontes Consultadas"):
                        for i, source in enumerate(sources, 1):
                            display_source(source, i)

                # Add to messages
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response,
                    "sources": sources
                })

                # Auto-save
                save_conversation(
                    st.session_state.current_chat_id,
                    st.session_state.messages,
                    st.session_state.user_name
                )

                st.rerun()

            except Exception as e:
                progress_placeholder.empty()
                progress_bar.empty()
                st.error(str(e))

        else:
            # Non-streaming response (c√≥digo existente)
            # ...
```

#### **PARTE 4: UI Aprimorada com CSS**

```python
# frontend/app.py

# Adicionar no CSS (ap√≥s linha 24)

st.markdown("""
<style>
    /* Existing CSS ... */

    /* Progress indicator styling */
    .progress-container {
        background: rgba(255, 255, 255, 0.05);
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        border: 1px solid rgba(102, 126, 234, 0.3);
    }

    .progress-stage {
        display: flex;
        align-items: center;
        margin: 0.5rem 0;
        font-size: 0.95rem;
        color: #667eea;
        font-weight: 500;
    }

    .progress-stage.active {
        color: #4ecdc4;
        font-weight: 600;
    }

    .progress-stage.completed {
        color: #95a5a6;
        opacity: 0.7;
    }

    .progress-icon {
        margin-right: 0.75rem;
        font-size: 1.2rem;
    }

    .progress-percentage {
        margin-left: auto;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 0.25rem 0.75rem;
        border-radius: 12px;
        font-size: 0.85rem;
        font-weight: 600;
    }

    /* Stage-specific colors */
    .stage-creating_llm { color: #3498db; }
    .stage-searching_books { color: #e74c3c; }
    .stage-building_context { color: #f39c12; }
    .stage-generating_answer { color: #2ecc71; }
    .stage-formatting_response { color: #9b59b6; }

    /* Animated progress bar */
    @keyframes progressAnimation {
        0% { width: 0%; }
        100% { width: 100%; }
    }

    .animated-progress {
        animation: progressAnimation 0.5s ease-out;
    }
</style>
""", unsafe_allow_html=True)
```

### UI Mockup (Como Deve Ficar)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ü§ñ Assistente                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                   ‚îÇ
‚îÇ  üîç Processando sua pergunta...                  ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ ‚úì Criando modelo LLM           [10%]      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚úì Buscando nos livros esp√≠ritas [30%]     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚úì Construindo contexto         [50%]      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚ñ∂ Gerando resposta              [70%]      ‚îÇ  ‚îÇ ‚Üê ATIVO
‚îÇ  ‚îÇ   Formatando resposta           [90%]      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 70%            ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ  [Texto sendo gerado aparece aqui...]‚ñå           ‚îÇ
‚îÇ                                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä Benef√≠cios Esperados

### Experi√™ncia do Usu√°rio
1. **Transpar√™ncia**: Usu√°rio sabe exatamente o que est√° acontecendo
2. **Confian√ßa**: Ver processo reduz ansiedade de espera
3. **Engajamento**: Visualiza√ß√£o torna espera mais interessante
4. **Profissionalismo**: Interface polida e moderna

### Alinhamento com Produto
1. **Cumpre Promessa**: "Estilo Perplexity" finalmente implementado
2. **Diferencial**: Poucos chatbots locais t√™m essa transpar√™ncia
3. **Educativo**: Usu√°rio aprende como sistema funciona

### T√©cnico
1. **Debugging**: Mais f√°cil identificar onde processo est√° travando
2. **Monitoramento**: M√©tricas por stage
3. **Feedback**: Usu√°rios podem reportar problemas espec√≠ficos por stage

## üéØ M√©tricas de Sucesso

### KPIs
1. **Satisfa√ß√£o**: Feedback sobre nova UI
   - Meta: +30% de coment√°rios positivos sobre interface
2. **Clareza**: % de usu√°rios que entendem o que est√° acontecendo
   - Meta: >90%
3. **Performance**: Lat√™ncia adicional da UI
   - Meta: <50ms overhead total

### Testes
- [ ] Testar todos os 5 stages aparecem corretamente
- [ ] Verificar transi√ß√µes suaves entre stages
- [ ] Validar barra de progresso atualiza corretamente
- [ ] Testar em diferentes navegadores
- [ ] Verificar responsividade mobile

## üìù Documenta√ß√£o a Atualizar

### README.md
```markdown
## üîç Interface em Tempo Real

O assistente mostra exatamente o que est√° fazendo:

### Est√°gios Vis√≠veis
1. **Criando modelo LLM** (10%) - Inicializando modelo de linguagem
2. **Buscando nos livros** (30%) - Procurando trechos relevantes
3. **Construindo contexto** (50%) - Organizando informa√ß√µes
4. **Gerando resposta** (70%) - LLM processando resposta
5. **Formatando resposta** (90%) - Preparando exibi√ß√£o

Voc√™ v√™ cada etapa em tempo real com barra de progresso!
```

## ‚öôÔ∏è Configura√ß√µes Ajust√°veis

### Progress Descriptions
```python
# backend/api_server.py
STAGE_DESCRIPTIONS = {
    "creating_llm": "Criando modelo LLM",
    "searching_books": "Buscando nos livros esp√≠ritas",
    "building_context": "Construindo contexto",
    "generating_answer": "Gerando resposta",
    "formatting_response": "Formatando resposta"
}
```

### UI Customization
```python
# frontend/app.py
SHOW_PROGRESS_BAR = True
SHOW_PERCENTAGE = True
SHOW_STAGE_ICONS = True
ANIMATE_TRANSITIONS = True
```

## üöÄ Rollout Sugerido

### Fase 1: Backend (0.5 dia)
- [ ] Modificar `query_stream()` para yield todos os stages
- [ ] Testar via curl que todos eventos s√£o enviados

### Fase 2: Frontend API (0.5 dia)
- [ ] Modificar `stream_api_response()` para parsear status
- [ ] Modificar signature para retornar tripla (chunk, sources, status)
- [ ] Testar parsing

### Fase 3: Frontend UI (2 dias)
- [ ] Adicionar progress bar e status display
- [ ] Implementar CSS styling
- [ ] Testar todas as anima√ß√µes
- [ ] Ajustar responsividade

### Fase 4: Polish (1 dia)
- [ ] Adicionar √≠cones por stage
- [ ] Melhorar anima√ß√µes
- [ ] Testar UX completa
- [ ] Documenta√ß√£o

## üîÑ Alternativas Consideradas

### Alternativa 1: Apenas Console Logs
**Pr√≥s**: Sem mudan√ßa de c√≥digo
**Contras**: Usu√°rio n√£o v√™ nada
**Decis√£o**: ‚ùå Rejeitada - n√£o atende requisito

### Alternativa 2: Polling de Status
**Pr√≥s**: Simples de implementar
**Contras**: Lat√™ncia, menos eficiente que streaming
**Decis√£o**: ‚ùå Rejeitada - j√° temos streaming

### Alternativa 3: WebSocket
**Pr√≥s**: Bi-direcional, mais moderno
**Contras**: Mais complexo, Streamlit j√° usa SSE
**Decis√£o**: ‚ùå Rejeitada - over-engineering

### Alternativa 4: Completar Implementa√ß√£o SSE Existente (Escolhida)
**Pr√≥s**: Usa infraestrutura existente, eficiente
**Contras**: Nenhum significativo
**Decis√£o**: ‚úÖ Adotada

## üîÆ Evolu√ß√µes Futuras (v2.0)

1. **Estimativa de Tempo**: "~5s restantes"
2. **Sub-stages**: Breakdown detalhado de cada stage
3. **Livros Sendo Consultados**: Mostrar quais livros durante busca
4. **Anima√ß√µes Avan√ßadas**: Transi√ß√µes mais suaves
5. **Dark/Light Mode**: Temas customiz√°veis
6. **Progress History**: Mostrar quanto tempo cada stage levou

## üìö Refer√™ncias

- [Streamlit Progress Bar](https://docs.streamlit.io/library/api-reference/status/st.progress)
- [Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [Perplexity AI UX](https://www.perplexity.ai/)

---

**Data de Cria√ß√£o**: 2025-02-01
**Autor**: Sistema de An√°lise
**Revis√£o**: Pendente
**Status**: üìù Proposta Inicial
