# ğŸ“š Assistente EspÃ­rita - Sistema RAG com PriorizaÃ§Ã£o de Fontes

Sistema de assistente conversacional especializado em Doutrina EspÃ­rita, utilizando RAG (Retrieval-Augmented Generation) com priorizaÃ§Ã£o inteligente das obras da CodificaÃ§Ã£o.

## ğŸ¯ CaracterÃ­sticas Principais

- **PriorizaÃ§Ã£o de Fontes**: Sistema que prioriza O Livro dos EspÃ­ritos sobre demais obras
- **Busca SemÃ¢ntica**: ChromaDB com embeddings multilÃ­ngues otimizados para portuguÃªs
- **LLM Local**: Ollama com modelo Llama 3.2:3b rodando localmente
- **Interface Web**: Streamlit com feedback colaborativo
- **Arquitetura Cliente-Servidor**: Backend local (GPU) + Frontend na nuvem

## ğŸ“‹ Hierarquia de Fontes

1. ğŸ¥‡ **Prioridade MÃ¡xima** (peso 100): O Livro dos EspÃ­ritos
2. ğŸ¥ˆ **Obras Fundamentais** (peso 70): Evangelho, MÃ©diuns, GÃªnese, CÃ©u e Inferno, O que Ã© o Espiritismo
3. ğŸ¥‰ **Complementar** (peso 40): Revista EspÃ­rita (1858-1869)
4. ğŸ“„ **Outras Obras** (peso 10): Demais livros

## ğŸ—ï¸ Arquitetura
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   UsuÃ¡rios      â”‚
    â”‚   (Internet)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Frontend (Cloud)      â”‚
    â”‚  Streamlit Cloud       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTPS/API
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ngrok Tunnel          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Backend (Local PC)    â”‚
    â”‚  â€¢ FastAPI             â”‚
    â”‚  â€¢ ChromaDB            â”‚
    â”‚  â€¢ Ollama (GPU)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ–¥ï¸ Stack TecnolÃ³gico

### Backend
- **FastAPI**: API REST
- **Ollama**: Servidor LLM local
- **Llama 3.2:3b**: Modelo de linguagem otimizado para portuguÃªs
- **ChromaDB**: Banco de dados vetorial
- **LangChain**: Framework RAG
- **Sentence Transformers**: Embeddings multilÃ­ngues
- **PyTorch**: GPU acceleration (CUDA)

### Frontend
- **Streamlit**: Interface web
- **Requests**: Cliente HTTP para API

### Infraestrutura
- **ngrok**: TÃºnel seguro para expor backend
- **Streamlit Cloud**: Hospedagem do frontend
- **GitHub**: Versionamento

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.11
- NVIDIA GPU com CUDA (recomendado)
- Ollama instalado
- ngrok account (para deploy)

### Backend Setup
```bash
cd backend

# Criar ambiente virtual
python -m venv venv
venv\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar PyTorch com CUDA (para GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Instalar Ollama
# Baixar de: https://ollama.com/download

# Baixar modelo
ollama pull llama3.2:3b

# Adicionar livros espÃ­ritas em PDF na pasta books/

# Processar livros (criar banco vetorial)
python process_books.py

# Iniciar API
python api_server.py
```

API estarÃ¡ disponÃ­vel em: `http://localhost:8000`

### Frontend Setup (Local)
```bash
cd frontend

# Criar ambiente virtual
python -m venv venv
venv\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar API URL
# Criar .streamlit/secrets.toml:
mkdir .streamlit
echo API_URL = "http://localhost:8000" > .streamlit\secrets.toml

# Rodar frontend
streamlit run app.py
```

Frontend estarÃ¡ disponÃ­vel em: `http://localhost:8501`

## ğŸš€ Deploy

### 1. Backend (ngrok)
```bash
cd backend

# Terminal 1: Rodar backend
python api_server.py

# Terminal 2: Expor via ngrok
ngrok http 8000

# Copiar URL gerada (ex: https://abc-123.ngrok-free.app)
```

### 2. Frontend (Streamlit Cloud)
```bash
cd frontend

# Inicializar Git
git init
git add .
git commit -m "Initial commit"

# Criar repositÃ³rio no GitHub
# Depois fazer push

# Deploy no Streamlit Cloud:
# 1. Acesse https://share.streamlit.io
# 2. New app
# 3. Conecte seu repositÃ³rio GitHub
# 4. Configure secrets:
#    API_URL = "https://sua-url-ngrok.ngrok-free.app"
# 5. Deploy!
```

## ğŸ“Š Sistema de Feedback

O sistema coleta feedback colaborativo de usuÃ¡rios para melhorar continuamente:

- **AvaliaÃ§Ãµes**: Boa (ğŸ‘), Regular (ğŸ˜), Ruim (ğŸ‘)
- **ComentÃ¡rios**: Feedback textual detalhado
- **Armazenamento**: JSONL local para anÃ¡lise
- **AnÃ¡lise**: Script `view_feedback.py` para visualizar estatÃ­sticas

### Visualizar Feedback
```bash
cd frontend
streamlit run view_feedback.py
```

## ğŸ§ª Testes Locais

### Testar Backend
```bash
# Verificar status
curl http://localhost:8000/

# Testar query
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "O que Ã© o perispÃ­rito?",
    "model_name": "llama3.2:3b",
    "temperature": 0.3,
    "top_k": 8,
    "fetch_k": 20
  }'

# Ver documentaÃ§Ã£o interativa
# Abrir no navegador: http://localhost:8000/docs
```

### Testar Frontend

1. Certifique-se que o backend estÃ¡ rodando
2. Execute `streamlit run app.py`
3. FaÃ§a perguntas de teste
4. Verifique as fontes retornadas
5. Teste o sistema de feedback

## ğŸ“ˆ Performance

### Benchmarks (RTX 3070 + Ryzen 7 5700x)

- **Processamento de livros**: ~15min para 22.868 chunks
- **Busca vetorial**: ~200ms para top 20 resultados
- **GeraÃ§Ã£o de resposta**: 2-5s (dependendo do tamanho)
- **Throughput**: ~10-15 perguntas/minuto

### OtimizaÃ§Ãµes Implementadas

- âœ… Processamento em batch (5000 chunks/vez)
- âœ… DeduplicaÃ§Ã£o de chunks similares
- âœ… Cache de embeddings (GPU)
- âœ… PriorizaÃ§Ã£o antes da inferÃªncia
- âœ… Reranking baseado em relevÃ¢ncia + prioridade

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Ajustar Prioridades

Edite `backend/config.py`:
```python
BOOK_PRIORITIES = {
    "livro-dos-espiritos.pdf": 100,  # Alterar peso aqui
    "evangelho": 70,
    # ...
}
```

### Trocar Modelo LLM
```bash
# Listar modelos disponÃ­veis
ollama list

# Baixar novo modelo
ollama pull llama3.2:1b

# Atualizar no frontend (sidebar)
model_name = st.selectbox(
    "Modelo:",
    ["llama3.2:3b", "llama3.2:1b", "llama3.2:1b"],  # Adicionar aqui
)
```

### Ajustar Chunk Size

Edite `backend/config.py`:
```python
CHUNK_SIZE = 1000      # Tamanho do chunk (caracteres)
CHUNK_OVERLAP = 200    # SobreposiÃ§Ã£o entre chunks
```

Depois reprocessar:
```bash
python process_books.py
```

## ğŸ“ Estrutura de Arquivos
```
chatbot-comeerj/
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ GUIA_TESTADORES.md          # Guia para testadores
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ api_server.py           # API FastAPI
â”‚   â”œâ”€â”€ config.py               # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ priority_retriever.py  # Sistema de priorizaÃ§Ã£o
â”‚   â”œâ”€â”€ feedback_system.py     # Sistema de feedback
â”‚   â”œâ”€â”€ process_books.py       # IndexaÃ§Ã£o de livros
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ start_backend.bat       # Script Windows para iniciar
â”‚   â”œâ”€â”€ start_with_ngrok.bat    # Script com ngrok
â”‚   â”œâ”€â”€ books/                  # PDFs (nÃ£o versionado)
â”‚   â”œâ”€â”€ database/               # ChromaDB (nÃ£o versionado)
â”‚   â””â”€â”€ feedback/               # Dados de feedback
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ app.py                  # Interface Streamlit
    â”œâ”€â”€ feedback_system.py     # Sistema de feedback
    â”œâ”€â”€ view_feedback.py       # AnÃ¡lise de feedback
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ .streamlit/
        â””â”€â”€ secrets.toml        # Config API URL (nÃ£o versionado)
```

## ğŸ› Troubleshooting

### Backend nÃ£o inicia
```bash
# Verificar se Ollama estÃ¡ rodando
ollama list

# Se nÃ£o, iniciar Ollama
ollama serve

# Verificar CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Reinstalar PyTorch com CUDA se necessÃ¡rio
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Frontend nÃ£o conecta
```bash
# Verificar se backend estÃ¡ rodando
curl http://localhost:8000/

# Verificar secrets.toml
cat frontend/.streamlit/secrets.toml

# Verificar ngrok (se usando)
# Acessar http://localhost:4040 para ver status do tÃºnel
```

### Banco vetorial corrompido
```bash
cd backend

# Deletar banco
rm -rf database/

# Reprocessar livros
python process_books.py
```

### Respostas de baixa qualidade

1. **Temperatura muito alta**: Reduzir para 0.1-0.3
2. **Poucos trechos**: Aumentar `top_k` para 10-12
3. **Busca limitada**: Aumentar `fetch_k` para 30-40
4. **Modelo inadequado**: Testar outros modelos (llama3.2:1b)

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Ollama](https://ollama.com/)
- [ChromaDB](https://docs.trychroma.com/)
- [LangChain](https://python.langchain.com/)
- [Streamlit](https://docs.streamlit.io/)

## ğŸ¤ Contribuindo

Este Ã© um projeto colaborativo com sistema de feedback integrado. Para contribuir:

1. Use o sistema normalmente
2. Avalie cada resposta (ğŸ‘ğŸ˜ğŸ‘)
3. Deixe comentÃ¡rios detalhados
4. Reporte bugs via issues

## ğŸ“„ LicenÃ§a

Este projeto Ã© para uso educacional e religioso. Os livros espÃ­ritas utilizados sÃ£o de domÃ­nio pÃºblico (obras de Allan Kardec).

## âœ¨ Agradecimentos

- Allan Kardec pela CodificaÃ§Ã£o EspÃ­rita
- FEB (FederaÃ§Ã£o EspÃ­rita Brasileira) pelas traduÃ§Ãµes
- Comunidade open-source pelos frameworks utilizados
- Testadores colaborativos pelo feedback

## ğŸ“§ Contato

Para dÃºvidas ou sugestÃµes sobre o projeto, entre em contato atravÃ©s dos issues do GitHub.

---

**VersÃ£o**: 1.0.0  
**Ãšltima atualizaÃ§Ã£o**: Janeiro 2025
